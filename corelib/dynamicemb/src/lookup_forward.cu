/******************************************************************************
# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
******************************************************************************/

#include "check.h"
#include "lookup_forward.h"
#include "lookup_kernel.cuh"

namespace dyn_emb {

template <typename SrcType, typename DstType, typename offset_t>
struct ForwardMultiToOneFMLayoutDesc {
  using SrcT = SrcType;
  using DstT = DstType;

  HOST_DEVICE_INLINE int get_offset(int i) {
    return offset_ptr[i] - offset_ptr[0];
  }
  HOST_DEVICE_INLINE int get_vec_length(int i) {
    // TODO:now only have one size
    return ev_size;
  }
  HOST_DEVICE_INLINE int get_average_pooling_factor(int i) {
    int pooling_factor = static_cast<int>(offset_ptr[i + 1] - offset_ptr[i]);
    // TODO:now use 1 = Average, 0 = SUM or None
    return combiner == 1 ? pooling_factor : 1;
  }
  HOST_DEVICE_INLINE const SrcType *get_src_ptr(int i) {
    int idx = reverse_idx_ptr[i];
    return src_ptr + ev_size * idx;
  }
  HOST_DEVICE_INLINE DstType *get_dst_ptr(int i) {
    int b = i % batch_size;
    int f = i / batch_size;
    return dst_ptr + b * total_D + accum_D + f * ev_size;
  }

  int num_vec_;
  int combiner;
  int ev_size;
  const offset_t *__restrict__ offset_ptr;
  const offset_t *__restrict__ reverse_idx_ptr;
  const SrcType *__restrict__ src_ptr;
  DstType *dst_ptr;
  int batch_size;
  int total_D;
  int accum_D;
};

void scatter_combine(void *src_ptr, void *dst_ptr, void *offset_ptr,
                     void *inverse_idx_ptr, int combiner, int total_D,
                     int accum_D, int ev_size, int num_vec, int batch_size,
                     DataType src_type, DataType dst_type, DataType offset_type,
                     cudaStream_t stream) {

  DISPATCH_INTEGER_DATATYPE_FUNCTION(offset_type, offset_t, [&] {
    DISPATCH_FLOAT_DATATYPE_FUNCTION(src_type, src_t, [&] {
      DISPATCH_FLOAT_DATATYPE_FUNCTION(dst_type, dst_t, [&] {
        using CopyDesc = ForwardMultiToOneFMLayoutDesc<src_t, dst_t, offset_t>;
        CopyDesc multi_to_one_desc{num_vec,
                                   combiner,
                                   ev_size,
                                   (offset_t *)offset_ptr,
                                   (offset_t *)inverse_idx_ptr,
                                   (src_t *)src_ptr,
                                   (dst_t *)dst_ptr,
                                   batch_size,
                                   total_D,
                                   accum_D};
        copy_multi_to_one(multi_to_one_desc, ev_size, stream);
      });
    });
  });
}

template <typename SrcType, typename DstType, typename offset_t>
struct ForwardSequenceCopyDesc {

  using SrcT = SrcType;
  using DstT = DstType;
  HOST_DEVICE_INLINE int get_vec_length() {
    // TODO:now only have one size
    return ev_size;
  }
  HOST_DEVICE_INLINE const SrcType *get_src_ptr(int i) {
    offset_t idx = reverse_idx_ptr[i];
    return src_ptr + idx * ev_size;
  }
  HOST_DEVICE_INLINE DstType *get_dst_ptr(int i) {
    offset_t offset = offset_ptr[0];
    return dst_ptr + (offset + i) * ev_size;
  }

  int num_vec_;
  int ev_size;
  const offset_t *__restrict__ offset_ptr;
  const offset_t *__restrict__ reverse_idx_ptr;
  const SrcType *__restrict__ src_ptr;
  DstType *dst_ptr;
};

void scatter(void *src_ptr, void *dst_ptr, void *offset_ptr,
             void *inverse_idx_ptr, int num_emb, int ev_size, DataType src_type,
             DataType dst_type, DataType offset_type, int device_num_sms,
             cudaStream_t stream) {
  DISPATCH_INTEGER_DATATYPE_FUNCTION(offset_type, offset_t, [&] {
    DISPATCH_FLOAT_DATATYPE_FUNCTION(src_type, src_t, [&] {
      DISPATCH_FLOAT_DATATYPE_FUNCTION(dst_type, dst_t, [&] {
        using CopyDesc = ForwardSequenceCopyDesc<src_t, dst_t, offset_t>;
        CopyDesc sequence_copy_desc{num_emb,
                                    ev_size,
                                    (offset_t *)offset_ptr,
                                    (offset_t *)inverse_idx_ptr,
                                    (src_t *)src_ptr,
                                    (dst_t *)dst_ptr};
        copy_one_to_one<CopyDesc>(sequence_copy_desc, ev_size, device_num_sms,
                                  stream);
      });
    });
  });
}

template <typename SrcType, typename DstType, typename offset_t>
struct ForwardSequenceFusedCopyDesc {

  using SrcT = SrcType;
  using DstT = DstType;
  HOST_DEVICE_INLINE int get_vec_length() {
    // TODO:now only have one size
    return ev_size;
  }
  HOST_DEVICE_INLINE const SrcType *get_src_ptr(int i) {
    offset_t idx = reverse_idx_ptr[i];
    return src_ptr + idx * ev_size;
  }
  HOST_DEVICE_INLINE DstType *get_dst_ptr(int i) {
    return dst_ptr + i * ev_size;
  }

  int num_vec_;
  int ev_size;
  const offset_t *__restrict__ reverse_idx_ptr;
  const SrcType *__restrict__ src_ptr;
  DstType *dst_ptr;
};

void scatter_fused(void *src_ptr, void *dst_ptr, void *inverse_idx_ptr,
                   int num_emb, int ev_size, DataType src_type,
                   DataType dst_type, DataType offset_type, int device_num_sms,
                   cudaStream_t stream) {
  DISPATCH_INTEGER_DATATYPE_FUNCTION(offset_type, offset_t, [&] {
    DISPATCH_FLOAT_DATATYPE_FUNCTION(src_type, src_t, [&] {
      DISPATCH_FLOAT_DATATYPE_FUNCTION(dst_type, dst_t, [&] {
        using CopyDesc = ForwardSequenceFusedCopyDesc<src_t, dst_t, offset_t>;
        CopyDesc sequence_copy_desc{num_emb, ev_size,
                                    (offset_t *)inverse_idx_ptr,
                                    (src_t *)src_ptr, (dst_t *)dst_ptr};
        copy_one_to_one<CopyDesc>(sequence_copy_desc, ev_size, device_num_sms,
                                  stream);
      });
    });
  });
}

template <typename SrcType, typename DstType> struct ForwardOneToOneCopyDesc {

  using SrcT = SrcType;
  using DstT = DstType;
  HOST_DEVICE_INLINE int get_vec_length() {
    // TODO:now only have one size
    return vec_length;
  }
  HOST_DEVICE_INLINE const SrcType *get_src_ptr(int i) {
    return src_ptr + i * vec_length;
  }
  HOST_DEVICE_INLINE DstType *get_dst_ptr(int i) {
    return dst_ptr + i * vec_length;
  }
  int num_vec_;
  int vec_length;
  const SrcType *__restrict__ src_ptr;
  DstType *dst_ptr;
};

void batched_vector_copy_device(void *src_ptr, void *dst_ptr, int batch_size,
                                int vec_length, DataType src_type,
                                DataType dst_type, int device_num_sms,
                                cudaStream_t stream) {
  DISPATCH_FLOAT_DATATYPE_FUNCTION(src_type, src_t, [&] {
    DISPATCH_FLOAT_DATATYPE_FUNCTION(dst_type, dst_t, [&] {
      using CopyDesc = ForwardOneToOneCopyDesc<src_t, dst_t>;
      CopyDesc sequence_dedup_copy_desc{batch_size, vec_length,
                                        (src_t *)src_ptr, (dst_t *)dst_ptr};
      copy_one_to_one<CopyDesc>(sequence_dedup_copy_desc, vec_length,
                                device_num_sms, stream);
    });
  });
}

void add_offset(void *src_ptr, void *dst_ptr, int idx, DataType src_type,
                DataType dst_type, cudaStream_t stream) {
  DISPATCH_INTEGER_DATATYPE_FUNCTION(src_type, src_t, [&] {
    DISPATCH_INTEGER_DATATYPE_FUNCTION(dst_type, dst_t, [&] {
      add_offset_kernel<src_t, dst_t>
          <<<1, 1, 0, stream>>>(reinterpret_cast<const src_t *>(src_ptr),
                                reinterpret_cast<dst_t *>(dst_ptr), idx);
    });
  });
  DEMB_CUDA_KERNEL_LAUNCH_CHECK();
}

void get_new_length_and_offsets(uint64_t *d_unique_offsets,
                                int64_t *d_table_offsets_in_feature,
                                int table_num, int64_t new_lengths_size,
                                int local_batch_size, DataType length_type,
                                DataType offset_type, void *new_offsets,
                                void *new_lenghths, cudaStream_t stream) {

  int block_size = 256;
  int grid_size = (new_lengths_size + block_size - 1) / block_size;
  DISPATCH_OFFSET_INT_TYPE(offset_type, offset_t, [&] {
    DISPATCH_OFFSET_INT_TYPE(length_type, length_t, [&] {
      get_new_length_and_offsets_kernel<offset_t, length_t>
          <<<grid_size, block_size, 0, stream>>>(
              d_unique_offsets, d_table_offsets_in_feature, table_num,
              new_lengths_size, local_batch_size,
              reinterpret_cast<offset_t *>(new_offsets),
              reinterpret_cast<length_t *>(new_lenghths));
    });
  });
  DEMB_CUDA_KERNEL_LAUNCH_CHECK();
}

template <typename EmbType, typename ScoreType>
__global__ void mask_embeddings_by_frequency_kernel(
    EmbType* embeddings_ptr, const ScoreType* scores_ptr,
    int num_embeddings, int embedding_dim,
    ScoreType frequency_threshold, int mask_dims) {
  
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  int warp_id = idx / 32;  
  int lane_id = idx % 32; 
  
  if (warp_id >= num_embeddings) return;
  
  ScoreType score = scores_ptr[warp_id];

  if (score < frequency_threshold) {
    int start_dim = embedding_dim - mask_dims;
    if (start_dim < 0) start_dim = 0;
    
    EmbType* embedding_base = embeddings_ptr + warp_id * embedding_dim;
    
    for (int dim = start_dim + lane_id; dim < embedding_dim; dim += 32) {
      embedding_base[dim] = static_cast<EmbType>(0.0);
    }
  }
}

void mask_embeddings_by_frequency(void *embeddings_ptr, void *scores_ptr,
                                  int num_embeddings, int embedding_dim,
                                  int frequency_threshold, int mask_dims,
                                  DataType emb_type, DataType score_type,
                                  int device_num_sms, cudaStream_t stream) {
  if (frequency_threshold <= 0 || mask_dims <= 0) {
    return;  // No masking needed
  }

  int num_warps_needed = num_embeddings; // each warp processes one embedding
  int block_size = 256;  // 8 warps per block
  int grid_size = (num_warps_needed + 7) / 8;  // 8 warps per block

  DISPATCH_FLOAT_DATATYPE_FUNCTION(emb_type, emb_t, [&] {
    DISPATCH_INTEGER_DATATYPE_FUNCTION(score_type, score_t, [&] {
      mask_embeddings_by_frequency_kernel<emb_t, score_t>
          <<<grid_size, block_size, 0, stream>>>(
              reinterpret_cast<emb_t*>(embeddings_ptr),
              reinterpret_cast<const score_t*>(scores_ptr),
              num_embeddings, embedding_dim,
              static_cast<score_t>(frequency_threshold), mask_dims);
    });
  });
  DEMB_CUDA_KERNEL_LAUNCH_CHECK();
}

} // namespace dyn_emb