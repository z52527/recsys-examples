from typing import List

import hstu_cuda_ops
import torch
from ops.length_to_offsets import length_to_complete_offsets


class _JaggedTensorOpFunction(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx,
        offsets_list: List[torch.Tensor],
        max_seqlens: List[int],
        max_seqlen: int,
        *values_list,
    ):
        if len(offsets_list) == 1:
            single_offsets = offsets_list[0]
            lengths = single_offsets[1:] - single_offsets[:-1]
            ctx.mark_non_differentiable(lengths)
            return values_list[0], lengths

        dim_list = [v.size(-1) for v in values_list]
        assert all(
            dim == dim_list[0] for dim in dim_list
        ), "All tensors must have the same value dimension"
        # if isinstance(max_seqlen, torch.Tensor):
        #     max_seqlen = int(max_seqlen.item())
        with torch.cuda.nvtx.range("Calculate merged offsets", color="purple"):
            merged_offsets = torch.sum(
                torch.stack(offsets_list), dim=0, dtype=offsets_list[0].dtype
            )

        with torch.cuda.nvtx.range("Calculate merged lengths", color="purple"):
            total_length = sum(v.size(0) for v in values_list)

            hidden_dim = values_list[0].size(-1)

            merged_lengths = torch.diff(merged_offsets)
            ctx.mark_non_differentiable(merged_lengths)

        with torch.cuda.nvtx.range("merged values mem alloc", color="purple"):
            merged_values = torch.empty(
                (total_length, hidden_dim),
                dtype=values_list[0].dtype,
                device=values_list[0].device,
            ).requires_grad_(True)
        batch_size = offsets_list[0].size(0) - 1

        device = values_list[0].device
        # max_grid_size = torch.cuda.get_device_properties(device).multi_processor_count
        device_properties = torch.cuda.get_device_properties(0)
        #todo:python side max_grid_size not work now, cuda side use cudaDeviceProp to get max_grid_size
        # max_grid_size = 
        # OPTIMIZER_BLOCKSIZE_VEC = 64
        BLOCK_SIZE = 256
        GRID_SIZE = int(device_properties.multi_processor_count * (device_properties.max_threads_per_multi_processor / BLOCK_SIZE))
        # import pdb; pdb.set_trace()
        with torch.cuda.nvtx.range("calculate seqlen_per_block", color="purple"):
            # the larger hidden_dim is, the smaller seqlen_per_block becomes
            seqlen_per_block = (
                8
                if hidden_dim <= 128
                else 4
                if hidden_dim <= 256
                else 2
                if hidden_dim <= 512
                else 1
            )
            blocks_per_batch = (max_seqlen + seqlen_per_block - 1) // seqlen_per_block
            num_tensors = len(offsets_list)
            total_blocks = batch_size * blocks_per_batch * num_tensors
            block_workloads = torch.empty(
                total_blocks,
                dtype=torch.int32,
                device=values_list[0].device,
            )
            #warp configuration: ensure not exceeding 1024 threads, each warp processes 1 sequence
            target_warps = min(32, max(1, seqlen_per_block)); 
            threads = min(BLOCK_SIZE, target_warps * 32);
            blocks = min(GRID_SIZE, total_blocks)

        with torch.cuda.nvtx.range("calculate blocks workload", color="purple"):
            hstu_cuda_ops.compute_block_workloads(
                offsets_list, seqlen_per_block, max_seqlen, block_workloads
            )
        with torch.cuda.nvtx.range("workload offset cumsum", color="orange"):
            workload_offset = length_to_complete_offsets(block_workloads)

        with torch.cuda.nvtx.range("Cpp part forward", color="purple"):
            hstu_cuda_ops.concat_2D_jagged_tensors_forward(
                values_list,
                offsets_list,
                seqlen_per_block,
                max_seqlen,
                total_blocks,
                blocks,
                threads,
                workload_offset,
                merged_values,
                merged_offsets,
            )

        # save tensor variables
        ctx.save_for_backward(merged_offsets, workload_offset, *offsets_list)
        # save non-tensor variables
        ctx.seqlen_per_block = seqlen_per_block
        ctx.max_seqlen = max_seqlen
        ctx.blocks = blocks
        ctx.threads = threads
        ctx.total_blocks = total_blocks
        ctx.input_shapes = [v.shape for v in values_list]
        return merged_values, merged_lengths

    @staticmethod
    def backward(ctx, grad_output, grad_lengths):
        # handle special case: when there's only one tensor, return gradient directly
        if len(ctx.saved_tensors) == 0:
            # len(offsets_list) == 1, return gradient directly
            return None, None, None, grad_output

        # get saved tensor variables
        merged_offsets, workload_offset, *offsets_list = ctx.saved_tensors
        # get saved non-tensor variables
        seqlen_per_block = ctx.seqlen_per_block
        max_seqlen = ctx.max_seqlen
        blocks = ctx.blocks
        threads = ctx.threads
        total_blocks = ctx.total_blocks
        grad_inputs = [torch.empty(shape, dtype=grad_output.dtype, device=grad_output.device) for shape in ctx.input_shapes]

        with torch.cuda.nvtx.range("CUDA Backward", color="red"):
            hstu_cuda_ops.concat_2D_jagged_tensors_backward(
                grad_output,
                grad_lengths,
                seqlen_per_block,
                max_seqlen,
                total_blocks,
                blocks,
                threads,
                workload_offset,
                grad_inputs,
                offsets_list,
                merged_offsets,
            )
        return (None, None, None, *grad_inputs)


def switch_to_contiguous_if_needed(x: torch.Tensor) -> torch.Tensor:
    if not torch.jit.is_scripting() and torch.compiler.is_compiling():
        # Tell Dynamo this data-dependent value is in the range (0, 10**9)
        torch._check(x.size(0) > 0)
        torch._check(x.size(0) < 10**9)
    if x.stride(-1) == 1:
        return x
    return x.contiguous()


def jagged_2D_tensor_concat(
    values_list: List[torch.Tensor],
    offsets_list: List[torch.Tensor],
    max_seqlens: List[int],
    max_seqlen: int,
):
    assert len(values_list) == len(offsets_list)
    assert all(values_list[0].dtype == v.dtype for v in values_list)
    assert all(values_list[0].device == v.device for v in values_list)
    # Todo: need to support non contiguous case in the future.
    # assert all(v.is_contiguous() for v in values_list)

    values_list = [switch_to_contiguous_if_needed(v) for v in values_list]

    # Handle case where values_list length > 128 by batching
    batch_size = 128
    if len(values_list) <= batch_size:
        return _JaggedTensorOpFunction.apply(
            offsets_list, max_seqlens, max_seqlen, *values_list
        )

    result_values, result_lengths = None, None

    for i in range(0, len(values_list), batch_size):
        end_idx = min(i + batch_size, len(values_list))
        batch_values = values_list[i:end_idx]
        batch_offsets = offsets_list[i:end_idx]
        batch_max_seqlens = max_seqlens[i:end_idx]
        batch_max_seqlen = max(batch_max_seqlens)

        batch_result = _JaggedTensorOpFunction.apply(
            batch_offsets, batch_max_seqlens, batch_max_seqlen, *batch_values
        )

        if result_values is None:
            # First batch
            result_values, result_lengths = batch_result
        else:
            # Merge with previous result using 2-tensor concat
            prev_offsets = length_to_complete_offsets(result_lengths)
            curr_offsets = length_to_complete_offsets(batch_result[1])

            prev_max_seqlen = torch.max(result_lengths).item()
            curr_max_seqlen = torch.max(batch_result[1]).item()

            # Merge two results
            result_values, result_lengths = _JaggedTensorOpFunction.apply(
                [prev_offsets, curr_offsets],
                [prev_max_seqlen, curr_max_seqlen],
                max(prev_max_seqlen, curr_max_seqlen),
                result_values,
                batch_result[0],
            )

    return result_values, result_lengths
