# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import random

import pytest
import torch
from megatron.core import parallel_state, tensor_parallel
from torchrec.sparse.jagged_tensor import KeyedJaggedTensor

import commons.utils.initialize as init
from modules.embedding import (
    DynamicShardedEmbeddingConfig,
    EmbeddingOptimizerParam,
    ShardedEmbedding,
    ShardedEmbeddingConfig,
)
from modules.metrics.metric_modules import (
    RetrievalTaskMetricWithSampling,
)
from ops.collective_ops import grouped_allgatherv_tensor_list
from commons.utils.tensor_initializer import NormalInitializer


@pytest.mark.parametrize("num_embeddings", [10000])
@pytest.mark.parametrize("embedding_dim", [8])
@pytest.mark.parametrize("tp", [1, 2])
def test_distributed_topk(num_embeddings: int, embedding_dim: int, tp: int, max_k=5):
    torch.backends.cudnn.allow_tf32 = False
    torch.backends.cuda.matmul.allow_tf32 = False
    init.initialize_distributed()
    init.initialize_model_parallel(1)
    init.set_random_seed(1234)
    device = torch.device(f"cuda:{torch.cuda.current_device()}")

    embedding_optimizer_param = EmbeddingOptimizerParam(
        optimizer_str="sgd", learning_rate=1e-5
    )
    emb_configs = [
        ShardedEmbeddingConfig(
            feature_names=["context0"],
            table_name="context0_table",
            vocab_size=num_embeddings,
            dim=embedding_dim,
            sharding_type="model_parallel",
            initializer=NormalInitializer(),
            optimizer_param=embedding_optimizer_param,
        ),
        ShardedEmbeddingConfig(
            feature_names=["context1"],
            table_name="context1_table",
            vocab_size=1000,
            dim=embedding_dim,
            sharding_type="data_parallel",
            initializer=NormalInitializer(),
            optimizer_param=embedding_optimizer_param,
        ),
        DynamicShardedEmbeddingConfig(
            feature_names=["item"],
            table_name="item_table",
            vocab_size=num_embeddings,
            dim=embedding_dim,
            initializer=NormalInitializer(),
            optimizer_param=embedding_optimizer_param,
            global_hbm_for_values=0,
        ),
    ]
    embedding = ShardedEmbedding(emb_configs)

    metric_module = RetrievalTaskMetricWithSampling(MAX_K=max_k)

    # Create the JaggedTensor
    with tensor_parallel.get_cuda_rng_tracker().fork("sharded-embedding-group-seed"):
        context0 = torch.randint(num_embeddings, (100000,), device=device)
        context1 = torch.randint(1000, (100000,), device=device)
        item = torch.randint(num_embeddings, (100000,), device=device)
        lengths = torch.tensor(
            [context0.numel(), context1.numel(), item.numel()], device=device
        )
        kjt = KeyedJaggedTensor.from_lengths_sync(
            keys=["context0", "context1", "item"],
            values=torch.concat([context0, context1, item]),
            lengths=lengths,
        )
        embedding(kjt)

    sum_seqlen = random.randint(0, 100)
    query_embeddings = torch.rand(sum_seqlen, embedding_dim, device=device)
    target_ids = torch.randint(num_embeddings, (sum_seqlen,), device=device)

    metric_module(query_embeddings, target_ids)
    eval_dict_all, global_topk_logits, global_topk_keys = metric_module.compute(
        embedding, "item_table"
    )

    (all_query_embeddings, all_target_ids), _ = grouped_allgatherv_tensor_list(
        [query_embeddings, target_ids],
        torch.tensor([target_ids.numel()], dtype=torch.int64, device=device),
        pg=parallel_state.get_data_parallel_group(with_context_parallel=True),
    )

    # 2. export local embedding
    keys_array, values_array = embedding.export_local_embedding("item_table")
    local_keys = torch.tensor(keys_array, device=device)
    local_values = torch.tensor(values_array, device=device)
    (global_keys, global_values), _ = grouped_allgatherv_tensor_list(
        [local_keys, local_values],
        torch.tensor([local_keys.numel()], dtype=torch.int64, device=device),
        pg=torch.distributed.group.WORLD,
    )

    all_logits = torch.mm(all_query_embeddings, global_values.T)
    ref_top_k_logits, ref_top_k_indices = torch.topk(
        all_logits,
        dim=1,
        k=max_k,
        sorted=True,
        largest=True,
    )  # (B, k,)

    ref_top_k_keys = global_keys[ref_top_k_indices]
    assert torch.allclose(
        ref_top_k_keys, global_topk_keys
    ), f"global topk keys {global_topk_keys} should match reference {ref_top_k_keys}"
    assert torch.allclose(
        ref_top_k_logits, global_topk_logits, rtol=1e-3, atol=1e-2
    ), f"global topk logits {global_topk_logits} should match reference {ref_top_k_logits}"

    init.destroy_global_state()
